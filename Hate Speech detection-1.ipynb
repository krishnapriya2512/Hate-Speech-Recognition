{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e44db38f",
   "metadata": {},
   "source": [
    "# Case Study Module 2: Hate Speech Detection\n",
    "\n",
    "### By: Sri Krishna Priya Kondapalli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e6ba28a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Krishnapriya\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "# Importing the necessary libraries\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "import unidecode\n",
    "import pickle\n",
    "import os\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c26d75",
   "metadata": {},
   "source": [
    "## 1. Initial Checks on the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39bb3561",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet\n",
       "0   1      0   @user when a father is dysfunctional and is s...\n",
       "1   2      0  @user @user thanks for #lyft credit i can't us...\n",
       "2   3      0                                bihday your majesty\n",
       "3   4      0  #model   i love u take with u all the time in ...\n",
       "4   5      0             factsguide: society now    #motivation"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading the dataset from local directory using pandas\n",
    "df_train = pd.read_csv(r\"P:\\University of munster\\Module-2_Data Management\\Case study-2\\TwitterData\\train.csv\", encoding='ISO-8859-1')\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "859c3622",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31957</th>\n",
       "      <td>31958</td>\n",
       "      <td>0</td>\n",
       "      <td>ate @user isz that youuu?ð??ð??ð??ð??ð??ð...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31958</th>\n",
       "      <td>31959</td>\n",
       "      <td>0</td>\n",
       "      <td>to see nina turner on the airwaves trying to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31959</th>\n",
       "      <td>31960</td>\n",
       "      <td>0</td>\n",
       "      <td>listening to sad songs on a monday morning otw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31960</th>\n",
       "      <td>31961</td>\n",
       "      <td>1</td>\n",
       "      <td>@user #sikh #temple vandalised in in #calgary,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31961</th>\n",
       "      <td>31962</td>\n",
       "      <td>0</td>\n",
       "      <td>thank you @user for you follow</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  label                                              tweet\n",
       "31957  31958      0  ate @user isz that youuu?ð??ð??ð??ð??ð??ð...\n",
       "31958  31959      0    to see nina turner on the airwaves trying to...\n",
       "31959  31960      0  listening to sad songs on a monday morning otw...\n",
       "31960  31961      1  @user #sikh #temple vandalised in in #calgary,...\n",
       "31961  31962      0                   thank you @user for you follow  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking the dataset\n",
    "df_train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e2cc15b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31962, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking the shape of the data\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a383e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 31962 entries, 0 to 31961\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   id      31962 non-null  int64 \n",
      " 1   label   31962 non-null  int64 \n",
      " 2   tweet   31962 non-null  object\n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 749.2+ KB\n"
     ]
    }
   ],
   "source": [
    "#Checking for datatype information and null values\n",
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc33e3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for duplicates in the dataset if any and dropping them\n",
    "df_train.drop_duplicates(inplace = True)\n",
    "\n",
    "# There are no duplicate values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a7f590",
   "metadata": {},
   "source": [
    "## 2. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "223d303e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intialising Lemmatizer and stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "# Defining a preprocessor of data cleaning\n",
    "def preprocessor(tweet):\n",
    "    \n",
    "    # Removal of user handles\n",
    "    tweet = re.sub('@[\\w\\-]+','', tweet)\n",
    "    \n",
    "    # Coverting the string into lower case\n",
    "    tweet = str(tweet).lower()\n",
    "    \n",
    "    tweet = re.sub('\\[.*?\\]','',tweet)\n",
    "    \n",
    "    # Removal of HTML linkups\n",
    "    tweet = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|''[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+','',tweet)\n",
    "    tweet = re.sub('<.*?>+', '', tweet)\n",
    "    \n",
    "    # Removal of punctuations\n",
    "    tweet = re.sub('[%s]' % re.escape(string.punctuation), '', tweet)\n",
    "    tweet = re.sub('\\n','',tweet)\n",
    "    tweet = re.sub('\\w*\\d\\w*', '', tweet)\n",
    "    \n",
    "    # Removal of stopwords\n",
    "    tweet = [word for word in tweet.split(' ') if word not in stopwords]\n",
    "    \n",
    "    #removal of greek characters\n",
    "    tweet = [' '.join([unidecode.unidecode(word) for word in str(t).split()]) if t is not None else t for t in tweet]\n",
    "    \n",
    "    #lemmetizing of tweets\n",
    "    tweet = [\" \".join(lemmatizer.lemmatize(word) for word in t.split()) for t in tweet]\n",
    "    \n",
    "    tweet = \" \".join(tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8990ba9a",
   "metadata": {},
   "source": [
    "**Documenting decisions:**\n",
    "\n",
    "**Duplicates:** Removed to prevent skewed results.                                                     \n",
    "**URLs and Mentions:** Removed to keep the focus on content rather than specific user references.     \n",
    "**Normalization (Lower Text and Lemmatization):** Performed to standardize text.                       \n",
    "**Stopwords and greek characters removal:** Applied to reduce noise, retaining relevant words for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8b67fd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>father dysfunctional selfish drag kid dysfun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>thanks lyft credit cant use cause dont offer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>bihday majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>model   love u take u time urd+- ddddd|d|d|</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>factsguide society    motivation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet\n",
       "0   1      0    father dysfunctional selfish drag kid dysfun...\n",
       "1   2      0    thanks lyft credit cant use cause dont offer...\n",
       "2   3      0                                     bihday majesty\n",
       "3   4      0      model   love u take u time urd+- ddddd|d|d|  \n",
       "4   5      0                   factsguide society    motivation"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Applying preprocessor on the tweet column\n",
    "df_train['tweet'] = df_train['tweet'].apply(preprocessor)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5687c8",
   "metadata": {},
   "source": [
    "## 3. Model Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d57ff11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assigning labels to Y as an array\n",
    "y = np.array(df_train['label'])\n",
    "\n",
    "#Converting the text to Bag of words using countvectorizer\n",
    "bog = CountVectorizer(ngram_range = (2,2))\n",
    "train_data = bog.fit_transform(df_train['tweet'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b362da6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# intialising the train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_data,y, test_size = 0.33, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a3aa8b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating logistic Regression instance and fitting the data\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4928c145",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9456769055745164"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicting the data using the classifier instance and checking the accuracy\n",
    "prediction2 = lr.predict(X_test)\n",
    "accuracy_score(y_test,prediction2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "960f9d70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(criterion=&#x27;entropy&#x27;, n_estimators=200)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(criterion=&#x27;entropy&#x27;, n_estimators=200)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(criterion='entropy', n_estimators=200)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the random forest classifier instance and fitting the data\n",
    "random_classifier = RandomForestClassifier(n_estimators = 200, criterion = 'entropy')\n",
    "random_classifier.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35f1c34e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9510807736063709"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicting the data using the classifier instance and checking the accuracy\n",
    "prediction3 = random_classifier.predict(X_test)\n",
    "accuracy_score(y_test,prediction3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e06b4da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the SGDClassifier instance and fitting the data\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "clf = SGDClassifier(loss='log_loss', random_state=1, max_iter=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a0b3915",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9555365946150929"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training and predicting the data using classifier and checking the accuracy\n",
    "clf.partial_fit(train_data, train_labels, classes=[0, 1])\n",
    "prediction4 = clf.predict(X_test)\n",
    "accuracy_score(y_test,prediction4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d73a8a0",
   "metadata": {},
   "source": [
    "## 4. Predicting on test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c2fab038",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31963</td>\n",
       "      <td>#studiolife #aislife #requires #passion #dedic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31964</td>\n",
       "      <td>@user #white #supremacists want everyone to s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31965</td>\n",
       "      <td>safe ways to heal your #acne!!    #altwaystohe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31966</td>\n",
       "      <td>is the hp and the cursed child book up for res...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31967</td>\n",
       "      <td>3rd #bihday to my amazing, hilarious #nephew...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                              tweet\n",
       "0  31963  #studiolife #aislife #requires #passion #dedic...\n",
       "1  31964   @user #white #supremacists want everyone to s...\n",
       "2  31965  safe ways to heal your #acne!!    #altwaystohe...\n",
       "3  31966  is the hp and the cursed child book up for res...\n",
       "4  31967    3rd #bihday to my amazing, hilarious #nephew..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing test.csv from local directory\n",
    "df_test = pd.read_csv(r\"P:\\University of munster\\Module-2_Data Management\\Case study-2\\TwitterData\\test.csv\", encoding = 'ISO-8859-1')\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "57bb5601",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying preprocessor for test.csv\n",
    "test = df_test['tweet'].apply(preprocessor)\n",
    "test = bog.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4cf622bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   tweet  predictions\n",
      "0      #studiolife #aislife #requires #passion #dedic...            0\n",
      "1       @user #white #supremacists want everyone to s...            0\n",
      "2      safe ways to heal your #acne!!    #altwaystohe...            0\n",
      "3      is the hp and the cursed child book up for res...            0\n",
      "4        3rd #bihday to my amazing, hilarious #nephew...            0\n",
      "...                                                  ...          ...\n",
      "17192  thought factory: left-right polarisation! #tru...            1\n",
      "17193  feeling like a mermaid Ã°ÂÂÂ #hairflip #nev...            0\n",
      "17194  #hillary #campaigned today in #ohio((omg)) &am...            0\n",
      "17195  happy, at work conference: right mindset leads...            0\n",
      "17196  my   song \"so glad\" free download!  #shoegaze ...            0\n",
      "\n",
      "[17197 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#Testing the model prediction with text.csv\n",
    "predict = random_classifier.predict(test)\n",
    "\n",
    "#Making predictions for tweets in test.csv\n",
    "df_test['predictions'] = predict\n",
    "print(df_test[['tweet', 'predictions']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "213b2b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   tweet  predictions2\n",
      "0      #studiolife #aislife #requires #passion #dedic...             0\n",
      "1       @user #white #supremacists want everyone to s...             0\n",
      "2      safe ways to heal your #acne!!    #altwaystohe...             0\n",
      "3      is the hp and the cursed child book up for res...             0\n",
      "4        3rd #bihday to my amazing, hilarious #nephew...             0\n",
      "...                                                  ...           ...\n",
      "17192  thought factory: left-right polarisation! #tru...             1\n",
      "17193  feeling like a mermaid Ã°ÂÂÂ #hairflip #nev...             0\n",
      "17194  #hillary #campaigned today in #ohio((omg)) &am...             0\n",
      "17195  happy, at work conference: right mindset leads...             0\n",
      "17196  my   song \"so glad\" free download!  #shoegaze ...             0\n",
      "\n",
      "[17197 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#Testing the model and making prediction with text.csv\n",
    "predict2 = clf.predict(test)\n",
    "df_test['predictions2'] = predict2\n",
    "print(df_test[['tweet', 'predictions2']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060904d2",
   "metadata": {},
   "source": [
    "**As SGDClassifier has highest accuracy and is giving better prediction. I will be using it for the further process**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20348a03",
   "metadata": {},
   "source": [
    "## 5. Serializing and Deserializing of objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d4359279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using pickle to seriealise - stopwords, classifier(SGDClassifier) and countVectorizer.\n",
    "file_dest = os.path.join('HateSpeechDetection', 'pkl_objects')\n",
    "if not os.path.exists(file_dest):\n",
    "    os.makedirs(file_dest)\n",
    "\n",
    "pickle.dump(stopwords, open(os.path.join(file_dest, 'stopwords.pkl'), 'wb'), protocol=4)   \n",
    "pickle.dump(clf, open(os.path.join(file_dest, 'classifier.pkl'), 'wb'), protocol=4)\n",
    "pickle.dump(bog, open(os.path.join(file_dest, 'vect_bog.pkl'), 'wb'), protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "63db2524",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Krishnapriya\\\\University of Munster\\\\Module 2'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking the currect directory\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3857d560",
   "metadata": {},
   "source": [
    "The below code is used to write preprocessor and CountVectorizer into a python file to reuse it whenever necessary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "53fd7f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting HateSpeechDetection/vectorizer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile HateSpeechDetection/vectorizer.py\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# Deserializing the pickle files \n",
    "cur_dir = os.path.dirname(__file__)\n",
    "stop = pickle.load(open(os.path.join(cur_dir, 'pkl_objects', 'stopwords.pkl'), 'rb'))\n",
    "vect = pickle.load(open(os.path.join(cur_dir, 'pkl_objects', 'vect_bog.pkl'), 'rb'))\n",
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocessor(tweet):\n",
    "    \n",
    "    # Removal of user handles\n",
    "    tweet = re.sub('@[\\w\\-]+','', tweet)\n",
    "    \n",
    "    # Coverting the string into lower case\n",
    "    tweet = str(tweet).lower()\n",
    "    \n",
    "    tweet = re.sub('\\[.*?\\]','',tweet)\n",
    "    \n",
    "    # Removal of HTML linkups\n",
    "    tweet = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|''[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+','',tweet)\n",
    "    tweet = re.sub('<.*?>+', '', tweet)\n",
    "    \n",
    "    # Removal of punctuations\n",
    "    tweet = re.sub('[%s]' % re.escape(string.punctuation), '', tweet)\n",
    "    tweet = re.sub('\\n','',tweet)\n",
    "    tweet = re.sub('\\w*\\d\\w*', '', tweet)\n",
    "    \n",
    "    # Removal of stopwords\n",
    "    tweet = [word for word in tweet.split(' ') if word not in stop]\n",
    "    \n",
    "    #removal of greek characters\n",
    "    tweet = [' '.join([unidecode.unidecode(word) for word in str(t).split()]) if t is not None else t for t in tweet]\n",
    "    \n",
    "    #lemmetizing of tweets\n",
    "    tweet = [\" \".join(lemmatizer.lemmatize(word) for word in t.split()) for t in tweet]\n",
    "    \n",
    "    tweet = \" \".join(tweet)\n",
    "    return tweet\n",
    "\n",
    "#vect = CountVectorizer()\n",
    "\n",
    "def process_tweet(tweet):\n",
    "    # Process the tweet\n",
    "    processed_tweet = preprocessor(tweet)\n",
    "\n",
    "    \n",
    "    vect.transform([processed_tweet])  # Pass a list of processed_tweet\n",
    "\n",
    "    return processed_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b7004808",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changing the current directory to Hate Speech Detection\n",
    "os.chdir('HateSpeechDetection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "29365ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the countVectorizer embedded into vectoriser and deserializing the classifier\n",
    "from vectorizer import vect\n",
    "\n",
    "clf = pickle.load(open(os.path.join('pkl_objects', 'classifier.pkl'), 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7c8d2c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Not Hate Speech\n",
      "Probability: 93.65%\n"
     ]
    }
   ],
   "source": [
    "# Testing if the Deserialization of objects is working fine.\n",
    "import numpy as np\n",
    "label = {0:'Not Hate Speech', 1:'Hate Speech'}\n",
    "\n",
    "example = [\"America is good in tech\"]\n",
    "X = vect.transform(example)\n",
    "\n",
    "prediction = clf.predict(X)\n",
    "probability = clf.predict_proba(X)\n",
    "\n",
    "print('Prediction: %s\\nProbability: %.2f%%' % (label[prediction[0]], np.max(probability) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67c2517",
   "metadata": {},
   "source": [
    "## Setting up SQLite connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "85e53417",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Krishnapriya\\\\University of Munster\\\\Module 2\\\\HateSpeechDetection'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting the currect directory\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7c067cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up SQLite3 connection\n",
    "\n",
    "conn = sqlite3.connect('speech.sqlite')\n",
    "c = conn.cursor()\n",
    "\n",
    "c.execute('DROP TABLE IF EXISTS speech_db')\n",
    "c.execute('CREATE TABLE speech_db (speech TEXT,type INTEGER, date TEXT)')\n",
    "\n",
    "example1 = 'America is a rasict country'\n",
    "c.execute(\"INSERT INTO speech_db (speech, type, date) VALUES (?, ?, DATETIME('now'))\", (example1, 1))\n",
    "\n",
    "example2 = 'america is good with tech'\n",
    "c.execute(\"INSERT INTO speech_db (speech, type, date) VALUES (?, ?, DATETIME('now'))\", (example2, 0))\n",
    "\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c52931",
   "metadata": {},
   "source": [
    "## End "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
